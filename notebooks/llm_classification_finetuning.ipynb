{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f93489c-6d0a-44b3-a8ee-871d7b865f1b",
   "metadata": {},
   "source": [
    "# LLM Classification Finetuning\n",
    "#### Author: James Coffey   \n",
    "#### Date: 2025‑08‑18\n",
    "#### Challenge URL: [LLM Classification Finetuning](https://www.kaggle.com/competitions/llm-classification-finetuning)\n",
    "\n",
    "# Discussion — why I pivoted, what I built, and what I’d do next\n",
    "\n",
    "I started from the official Keras starter (JAX backend,\n",
    "**DeBERTa-v3-extra-small**, 512 tokens). That baseline put me at **1.03627** on\n",
    "the public leaderboard. My plan was to “do everything right”: move to bigger\n",
    "encoders (DeBERTa-Small/Base), increase max tokens, run **5-fold CV** with\n",
    "ensembling, add **A/B order augmentation**, train **more epochs**, and tune a\n",
    "cosine LR schedule.\n",
    "\n",
    "Very quickly, the hardware reality on Kaggle caught up with me:\n",
    "\n",
    "* **DeBERTa-Base** OOM’d on a P100.\n",
    "* **DeBERTa-Small** at longer context (≈640) with **5 folds × 6 epochs** was\n",
    "  \\~**48–49 hours** end-to-end, both as **JAX on P100** and **TF on 2×T4**. I\n",
    "  squeezed all the obvious knobs: pre-tokenized once per fold, **single backbone\n",
    "  pass** per step (A/B stacked), static shapes (drop\\_remainder), offline A/B\n",
    "  swap augmentation, mixed precision, and a tuned cosine LR. The wall-clock was\n",
    "  still dominated by raw kernel throughput.\n",
    "* JAX on a single **P100 (Pascal)** didn’t shine here (no Tensor Cores), and\n",
    "  while TF+MirroredStrategy on **2×T4** helped utilization, the total time for\n",
    "  **5×CV** was still not submission-friendly. I even hit a\n",
    "  `LossScaleOptimizer/merge_call` pitfall when mixing custom control-flow and\n",
    "  MP; disabling MP avoided the error but not the timeline.\n",
    "* I considered TPUs but skipped them—they’re fragile on Kaggle and I wanted\n",
    "  something reproducible.\n",
    "\n",
    "Given the constraints, I **pivoted**: instead of fine-tuning encoders for days,\n",
    "I’d **use a stronger pretrained model and a longer context, with no training at\n",
    "all**. Concretely, I moved to a **Llama-3 8B sequence-classification\n",
    "checkpoint** and pushed the **context to 4096 tokens** so I lose far less\n",
    "content from the prompt/responses. The upside is immediate: inference on 2×T4 is\n",
    "minutes, not days, and large LLM classifiers already encode robust preference\n",
    "signals when you give them enough context.\n",
    "\n",
    "## Final implementation (what this notebook does)\n",
    "\n",
    "* **Single model**: **Llama-3 8B** *sequence classifier* (no Gemma, no\n",
    "  sentence-transformers).\n",
    "* **Longer context**: **4096 tokens** (≫512).\n",
    "* **Symmetry trick**: I run on the original inputs and on a version with **A/B\n",
    "  swapped**, flip the swapped logits back, and **average**. This stabilizes A/B\n",
    "  preference predictions without any training.\n",
    "* **Two-GPU friendly**: I set `device_map=\"auto\"` so the checkpoint shards\n",
    "  across **2×T4** if available; otherwise it runs on one GPU.\n",
    "* **Completely offline**: The notebook reads the tokenizer and weights from an\n",
    "  attached `/kaggle/input/...` checkpoint and sets `local_files_only=True` so\n",
    "  there are **no internet calls**.\n",
    "\n",
    "  * I also prepared an **offline install recipe** (separate cell set) to pin\n",
    "    **torch==2.8.0 + xformers==0.0.32.post2 + triton==3.4.0** from a local\n",
    "    wheels dataset if I want the faster **human\\_pref + var-len + pipeline**\n",
    "    path. But the **default version here** uses only stock PyTorch/Transformers\n",
    "    and avoids `xformers` altogether for maximum submission safety.\n",
    "\n",
    "This gives me a strong, reproducible baseline that runs in a few minutes on\n",
    "2×T4. In earlier experiments with a similar setup, the inference-only approach\n",
    "brought me **down to 0.84236 on the public Leaderboard** in \\~5 minutes—orders of\n",
    "magnitude faster than training 5×CV.\n",
    "\n",
    "## Why this trade-off makes sense here\n",
    "\n",
    "* With the available GPUs, **full fine-tuning at scale isn’t time-feasible**\n",
    "  (even after optimization).\n",
    "* **Bigger pretrained models + longer context** recover a lot of the performance\n",
    "  we were chasing with training, **without** paying the multi-fold, multi-epoch\n",
    "  cost.\n",
    "* The result is **simple, robust, and offline-friendly**, which matters for\n",
    "  Kaggle submissions.\n",
    "\n",
    "## What I’d try next (if I had more compute or time)\n",
    "\n",
    "* **Lightweight tuning on the LLM classifier** (LoRA/QLoRA for a few epochs) to\n",
    "  squeeze extra points without the full 5×CV bill.\n",
    "* **Prompt/format tweaks** and **calibration** (e.g.,\n",
    "  temperature/label-smoothing on logits) to better model ties or reduce A/B\n",
    "  bias.\n",
    "* If stable, the **human\\_pref var-len pipeline** (Flash-Attn v3 via xFormers)\n",
    "  with the **offline wheels** improves throughput further.\n",
    "* A **distillation pass**: use the Llama-3 predictions to train a smaller\n",
    "  encoder (e.g., DeBERTa-Small) for cheap fast inference.\n",
    "* If compute becomes available, revisit the original plan: **5-fold CV +\n",
    "  ensembling** with longer context on a stronger encoder, now that the input\n",
    "  pipeline and schedule are dialed in.\n",
    "\n",
    "**Bottom line:** I explored the full fine-tune route, measured the bottlenecks,\n",
    "and chose a pragmatic pivot that fits Kaggle’s runtime and offline constraints\n",
    "while still delivering a big jump in score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9310d37d-d912-41f7-a757-b92ffc3fc637",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7290623f-b098-4ba9-84aa-ebb88ce9b122",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T21:59:20.466950Z",
     "iopub.status.busy": "2025-08-18T21:59:20.466539Z",
     "iopub.status.idle": "2025-08-18T21:59:25.848990Z",
     "shell.execute_reply": "2025-08-18T21:59:25.847908Z",
     "shell.execute_reply.started": "2025-08-18T21:59:20.466923Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Remove packages that would force Torch back to 2.6.0 during dependency resolution\n",
    "%pip uninstall -y torchvision torchaudio || true\n",
    "\n",
    "# Point to your attached wheels dataset under /kaggle/input\n",
    "PACKAGES_DIR = \"/kaggle/input/offline-pytorch280-xformers032/wheels\"  # ← change to your dataset path\n",
    "\n",
    "# Install strictly from local wheels (no internet)\n",
    "%pip install --no-index --find-links=$PACKAGES_DIR \\\n",
    "    torch==2.8.0 xformers==0.0.32.post2 triton==3.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bff44b57-9989-4965-a082-cb7c6fa663c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T21:59:29.311782Z",
     "iopub.status.busy": "2025-08-18T21:59:29.311056Z",
     "iopub.status.idle": "2025-08-18T21:59:29.487535Z",
     "shell.execute_reply": "2025-08-18T21:59:29.486307Z",
     "shell.execute_reply.started": "2025-08-18T21:59:29.311747Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/lmsys-modules-0805 human_pref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aecc067-5a86-48dc-89fb-85ddb70fb59f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T21:59:30.662220Z",
     "iopub.status.busy": "2025-08-18T21:59:30.661866Z",
     "iopub.status.idle": "2025-08-18T21:59:30.675215Z",
     "shell.execute_reply": "2025-08-18T21:59:30.674309Z",
     "shell.execute_reply.started": "2025-08-18T21:59:30.662188Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.8.0+cu128\n",
      "Transformers: 4.52.4\n",
      "CUDA devices: 2\n",
      "GPU 0: Tesla T4\n",
      "GPU 1: Tesla T4\n",
      "\n",
      "Attached inputs under /kaggle/input:\n",
      " • /kaggle/input/deberta_v3\n",
      " • /kaggle/input/llm-classification-finetuning\n",
      " • /kaggle/input/lmsys-checkpoints-3-0805\n",
      " • /kaggle/input/lmsys-modules-0805\n",
      " • /kaggle/input/offline-pytorch280-xformers032\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import textwrap\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "print(\"CUDA devices:\", torch.cuda.device_count())\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}:\", torch.cuda.get_device_name(i))\n",
    "\n",
    "# Small speed hints\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# Offline inference only (no hub calls)\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "\n",
    "# Paths to your attached datasets (update if your names differ)\n",
    "MODEL_DIR = \"/kaggle/input/lmsys-checkpoints-3-0805\"\n",
    "MODULES_DIR = \"/kaggle/input/lmsys-modules-0805\"\n",
    "\n",
    "print(\"\\nAttached inputs under /kaggle/input:\")\n",
    "for p in sorted(glob.glob(\"/kaggle/input/*\")):\n",
    "    print(\" •\", p)\n",
    "\n",
    "if not os.path.isdir(MODEL_DIR):\n",
    "    raise FileNotFoundError(\n",
    "        textwrap.dedent(f\"\"\"\n",
    "        MODEL_DIR not found: {MODEL_DIR}\n",
    "        → Use the right sidebar → Add data and attach the Llama‑3 8B classifier checkpoint dataset.\n",
    "    \"\"\")\n",
    "    )\n",
    "\n",
    "# Make `human_pref` importable\n",
    "if os.path.isdir(MODULES_DIR) and MODULES_DIR not in sys.path:\n",
    "    sys.path.insert(0, MODULES_DIR)\n",
    "    print(\"Using helper modules from:\", MODULES_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d49034e-f22f-4379-bc0a-30a8638ddd05",
   "metadata": {},
   "source": [
    "# Prepare test files (original + swapped A/B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68c7f867-d525-495e-97cf-126be8a3d19f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T21:51:53.336108Z",
     "iopub.status.busy": "2025-08-18T21:51:53.335331Z",
     "iopub.status.idle": "2025-08-18T21:51:53.342189Z",
     "shell.execute_reply": "2025-08-18T21:51:53.341224Z",
     "shell.execute_reply.started": "2025-08-18T21:51:53.336076Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting prepare_test_file.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile prepare_test_file.py\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\n",
    "# (Not scored here, but some helpers expect these columns.)\n",
    "for k in (\"winner_model_a\", \"winner_model_b\", \"winner_tie\"):\n",
    "    if k not in df.columns:\n",
    "        df[k] = 0\n",
    "\n",
    "df.to_parquet(\"test.parquet\", index=False)\n",
    "\n",
    "# Create swapped version (A<->B) for symmetry\n",
    "sw = df.copy()\n",
    "sw[\"response_a\"], sw[\"response_b\"] = sw[\"response_b\"], sw[\"response_a\"]\n",
    "sw.to_parquet(\"test_swap.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "440ae909-481b-415b-a4a0-548652a5fdb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T21:51:53.774359Z",
     "iopub.status.busy": "2025-08-18T21:51:53.774045Z",
     "iopub.status.idle": "2025-08-18T21:51:54.546519Z",
     "shell.execute_reply": "2025-08-18T21:51:54.545629Z",
     "shell.execute_reply.started": "2025-08-18T21:51:53.774334Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!python prepare_test_file.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eace29e-6c82-4048-a5d9-60d8de99e324",
   "metadata": {},
   "source": [
    "#  Llama‑3 8B classifier inference (4k context, offline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c56058e-9413-4e13-bec1-9af07cb8c79c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T21:51:56.370626Z",
     "iopub.status.busy": "2025-08-18T21:51:56.370260Z",
     "iopub.status.idle": "2025-08-18T21:51:56.379993Z",
     "shell.execute_reply": "2025-08-18T21:51:56.379003Z",
     "shell.execute_reply.started": "2025-08-18T21:51:56.370595Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting predict_llama.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile predict_llama.py\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from xformers.ops.fmha.attn_bias import BlockDiagonalCausalMask\n",
    "\n",
    "from human_pref.models.modeling_llama import LlamaForSequenceClassification\n",
    "from human_pref.data.processors import ProcessorPAB\n",
    "from human_pref.data.dataset import LMSYSDataset\n",
    "from human_pref.data.collators import VarlenCollator, ShardedMaxTokensCollator\n",
    "from human_pref.utils import to_device\n",
    "\n",
    "MODEL_DIR = os.getenv(\"MODEL_DIR\", \"/kaggle/input/lmsys-checkpoints-3-0805\")\n",
    "MAX_LENGTH = int(os.getenv(\"MAX_LENGTH\", \"4096\"))  # longer context\n",
    "BATCH_SIZE = int(\n",
    "    os.getenv(\"BATCH_SIZE\", \"80\")\n",
    ")  # loader batch (split into micro-batches)\n",
    "MAX_TOKENS = int(os.getenv(\"MAX_TOKENS\", \"8192\"))  # sharded tokens budget\n",
    "NUM_WORKERS = int(os.getenv(\"NUM_WORKERS\", \"4\"))\n",
    "DTYPE = torch.float16\n",
    "\n",
    "assert torch.cuda.is_available(), \"GPU is required. Enable 2×T4 in Kaggle settings.\"\n",
    "\n",
    "# ----------------------------------\n",
    "# Data pipeline (var‑len friendly)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "def make_loader(parquet_path: str):\n",
    "    \"\"\"Builds a var-length DataLoader for the LMSYS test parquet.\n",
    "\n",
    "    This constructs a tokenizer + `ProcessorPAB` that formats each example\n",
    "    (prompt, response_a, response_b) for Llama-style classification with long\n",
    "    context. It then creates an `LMSYSDataset` and a\n",
    "    `ShardedMaxTokensCollator`, which packs variable-length sequences into\n",
    "    micro-batches under a fixed token budget (`MAX_TOKENS`) for better GPU\n",
    "    utilization.\n",
    "\n",
    "    Args:\n",
    "      parquet_path: Absolute path to a `.parquet` file (e.g., \"test.parquet\"\n",
    "        or \"test_swap.parquet\") containing the competition columns:\n",
    "        [\"prompt\", \"response_a\", \"response_b\", ...].\n",
    "\n",
    "    Returns:\n",
    "      A `torch.utils.data.DataLoader` whose iterator yields **lists** of\n",
    "      micro-batches. Each micro-batch is a dict with keys required by the\n",
    "      var-length attention path:\n",
    "        - \"input_ids\": LongTensor of concatenated token ids\n",
    "        - \"cu_seqlens\": prefix sums of sequence lengths\n",
    "        - \"position_ids\": per-token rotary positions\n",
    "        - \"max_seq_len\": int (maximum sequence length in the micro-batch)\n",
    "        - \"seq_lens\": lengths of each sequence in the micro-batch\n",
    "    \"\"\"\n",
    "    # Load tokenizer from the local checkpoint directory.\n",
    "    tok = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "\n",
    "    # Suppress the benign warning when sequences exceed the model's original max.\n",
    "    tok.deprecation_warnings[\"sequence-length-is-longer-than-the-specified-maximum\"] = (\n",
    "        True\n",
    "    )\n",
    "\n",
    "    # Pack (prompt, A, B) into a single sequence suitable for sequence classification.\n",
    "    proc = ProcessorPAB(tokenizer=tok, max_length=MAX_LENGTH, support_system_role=True)\n",
    "\n",
    "    # Dataset reads rows from parquet and defers heavy work to the processor.\n",
    "    ds = LMSYSDataset(\n",
    "        csv_file=parquet_path,\n",
    "        query=None,\n",
    "        processor=proc,\n",
    "        include_swap=False,\n",
    "        is_parquet=True,\n",
    "    )\n",
    "\n",
    "    # Dynamic batching by tokens (not by examples) to keep memory usage stable.\n",
    "    coll = ShardedMaxTokensCollator(\n",
    "        max_tokens=MAX_TOKENS, base_collator=VarlenCollator()\n",
    "    )\n",
    "\n",
    "    # The DataLoader yields a list of micro-batches per outer batch, ready for pipeline run.\n",
    "    return DataLoader(\n",
    "        ds, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, collate_fn=coll\n",
    "    )\n",
    "\n",
    "\n",
    "# ----------------------------------\n",
    "# Model across 2 GPUs (pipeline split)\n",
    "# ----------------------------------\n",
    "\n",
    "n_gpus = torch.cuda.device_count()\n",
    "if n_gpus == 0:\n",
    "    raise SystemError(\"No GPU available. Please enable 2×T4.\")\n",
    "\n",
    "# 32 transformer layers for Llama‑3 8B\n",
    "NUM_LAYERS = 32\n",
    "if n_gpus >= 2:\n",
    "    device_map = {\n",
    "        \"model.embed_tokens\": \"cuda:0\",\n",
    "        \"model.norm\": \"cuda:1\",\n",
    "        \"score\": \"cuda:1\",\n",
    "    }\n",
    "    for i in range(NUM_LAYERS // 2):\n",
    "        device_map[f\"model.layers.{i}\"] = \"cuda:0\"\n",
    "    for i in range(NUM_LAYERS // 2, NUM_LAYERS):\n",
    "        device_map[f\"model.layers.{i}\"] = \"cuda:1\"\n",
    "else:\n",
    "    device_map = {\"\": \"cuda:0\"}\n",
    "\n",
    "model = LlamaForSequenceClassification.from_pretrained(\n",
    "    MODEL_DIR, torch_dtype=DTYPE, device_map=device_map\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# Build RoPE inv_freq per device (one per pipeline stage)\n",
    "cfg = model.config\n",
    "head_dim = cfg.hidden_size // cfg.num_attention_heads\n",
    "inv = 1.0 / (\n",
    "    cfg.rope_theta ** (torch.arange(0, head_dim, 2, dtype=torch.float32) / head_dim)\n",
    ")\n",
    "inv0 = inv.to(\"cuda:0\")\n",
    "inv1 = inv.to(\"cuda:1\" if n_gpus >= 2 else \"cuda:0\")\n",
    "\n",
    "# ----------------------------------\n",
    "# Pipeline run (micro‑batches)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "def run_one(parquet_path: str) -> torch.Tensor:\n",
    "    \"\"\"Runs pipelined two-GPU inference over one parquet file and returns probs.\n",
    "\n",
    "    This implements a simple two-stage pipeline across 2×T4 when available:\n",
    "    stage-0 runs `forward_part1` (lower layers) on GPU0 while stage-1 runs\n",
    "    `forward_part2` (upper layers + classifier) on GPU1. The first micro-batch\n",
    "    \"primes\" the pipeline; the last micro-batch is \"flushed\" at the end.\n",
    "\n",
    "    Args:\n",
    "      parquet_path: Absolute path to the `.parquet` test file to score.\n",
    "\n",
    "    Returns:\n",
    "      A float32 `torch.Tensor` of shape `(N, 3)` on CPU containing softmax\n",
    "      probabilities for `[winner_model_a, winner_model_b, winner_tie]` in the\n",
    "      same order as rows in `parquet_path`.\n",
    "    \"\"\"\n",
    "    loader = make_loader(parquet_path)\n",
    "    outs = []\n",
    "    is_first = True  # True until I prime stage-1 with the first micro-batch\n",
    "    prev_hidden = None  # Activations handed from stage-0 -> stage-1\n",
    "    prev_info = None  # Attention/position metadata for the prior micro-batch\n",
    "\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(dtype=DTYPE):\n",
    "        for batch in loader:\n",
    "            # Each `batch` is a list of micro-batches produced by the sharded collator.\n",
    "            for micro in batch:\n",
    "                # Stage-0 always runs on cuda:0.\n",
    "                input_ids = to_device(micro[\"input_ids\"], \"cuda:0\")\n",
    "                info = dict(\n",
    "                    cu_seqlens=micro[\"cu_seqlens\"],\n",
    "                    position_ids=micro[\"position_ids\"],\n",
    "                    max_seq_len=micro[\"max_seq_len\"],\n",
    "                    # Block-diagonal mask for packed variable-length sequences.\n",
    "                    attn_bias=BlockDiagonalCausalMask.from_seqlens(micro[\"seq_lens\"]),\n",
    "                )\n",
    "                info = to_device(info, \"cuda:0\")\n",
    "\n",
    "                if is_first:\n",
    "                    # Prime the pipeline: produce hidden states on stage-0,\n",
    "                    # then move them (and metadata) to stage-1's device.\n",
    "                    prev_hidden = model.forward_part1(input_ids, info, inv0)\n",
    "                    prev_info, prev_hidden = to_device(\n",
    "                        [info, prev_hidden], \"cuda:1\" if n_gpus >= 2 else \"cuda:0\"\n",
    "                    )\n",
    "                    is_first = False\n",
    "                    continue\n",
    "\n",
    "                # While stage-1 finishes the previous micro-batch...\n",
    "                logits = model.forward_part2(prev_hidden, prev_info, inv1)\n",
    "                # ...stage-0 concurrently starts the current micro-batch.\n",
    "                hidden = model.forward_part1(input_ids, info, inv0)\n",
    "\n",
    "                # Slide the pipeline window forward and stash logits.\n",
    "                prev_info, prev_hidden = to_device(\n",
    "                    [info, hidden], \"cuda:1\" if n_gpus >= 2 else \"cuda:0\"\n",
    "                )\n",
    "                outs.append(logits.cpu())\n",
    "\n",
    "        # Flush the final micro-batch through stage-1 after the loop.\n",
    "        if prev_hidden is not None:\n",
    "            logits = model.forward_part2(prev_hidden, prev_info, inv1)\n",
    "            outs.append(logits.cpu())\n",
    "\n",
    "    if not outs:\n",
    "        # Empty input file or all examples filtered out.\n",
    "        return torch.empty((0, 3))\n",
    "\n",
    "    pred = torch.cat(outs, dim=0)\n",
    "    return pred.softmax(-1)  # (N, 3)\n",
    "\n",
    "\n",
    "# Original & swapped, then flip A/B back and average\n",
    "prob_a = run_one(\"test.parquet\")\n",
    "prob_b = run_one(\"test_swap.parquet\")\n",
    "prob = (prob_a + prob_b[:, [1, 0, 2]]) / 2.0\n",
    "\n",
    "np.save(\"prob_llama.npy\", prob.cpu().numpy())\n",
    "print(\"Saved prob_llama.npy with shape:\", prob.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72b95227-1d66-4ee5-85c1-e7a93bb8d945",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T21:51:59.055742Z",
     "iopub.status.busy": "2025-08-18T21:51:59.055395Z",
     "iopub.status.idle": "2025-08-18T21:54:09.291488Z",
     "shell.execute_reply": "2025-08-18T21:54:09.290617Z",
     "shell.execute_reply.started": "2025-08-18T21:51:59.055715Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-18 21:52:05.634052: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755553925.663983    1077 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755553925.675338    1077 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [01:48<00:00, 27.09s/it]\n",
      "/kaggle/working/predict_llama.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast(dtype=DTYPE):\n",
      "/kaggle/working/predict_llama.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast(dtype=DTYPE):\n",
      "Saved prob_llama.npy with shape: torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Run inference\n",
    "!python predict_llama.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9ef917-fed4-486d-8bb4-45cc740f3373",
   "metadata": {},
   "source": [
    "# Make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8ba600-9c24-45b9-b703-e873fe5444ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T21:54:09.293477Z",
     "iopub.status.busy": "2025-08-18T21:54:09.293107Z",
     "iopub.status.idle": "2025-08-18T21:54:09.773115Z",
     "shell.execute_reply": "2025-08-18T21:54:09.772233Z",
     "shell.execute_reply.started": "2025-08-18T21:54:09.293447Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136060</td>\n",
       "      <td>0.002493</td>\n",
       "      <td>0.983723</td>\n",
       "      <td>0.013783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211333</td>\n",
       "      <td>0.565638</td>\n",
       "      <td>0.136214</td>\n",
       "      <td>0.298148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1233961</td>\n",
       "      <td>0.107226</td>\n",
       "      <td>0.728414</td>\n",
       "      <td>0.164360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  winner_model_a  winner_model_b  winner_tie\n",
       "0   136060        0.002493        0.983723    0.013783\n",
       "1   211333        0.565638        0.136214    0.298148\n",
       "2  1233961        0.107226        0.728414    0.164360"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"test.parquet\")  # ids from non‑swapped\n",
    "prob = np.load(\"prob_llama.npy\")\n",
    "\n",
    "sub = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": df[\"id\"],\n",
    "        \"winner_model_a\": prob[:, 0],\n",
    "        \"winner_model_b\": prob[:, 1],\n",
    "        \"winner_tie\": prob[:, 2],\n",
    "    }\n",
    ")\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a8bdfc",
   "metadata": {
    "papermill": {
     "duration": 0.761993,
     "end_time": "2024-10-16T17:35:46.196467",
     "exception": false,
     "start_time": "2024-10-16T17:35:45.434474",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Reference\n",
    "\n",
    "* [[HCMUS][2025][24C15034] Ensemble Inference](https://www.kaggle.com/code/hoangvu132/hcmus-2025-24c15034-ensemble-inference)\n",
    "* [LMSYS: KerasNLP Starter](https://www.kaggle.com/code/addisonhoward/lmsys-kerasnlp-starter)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9809560,
     "isSourceIdPinned": false,
     "sourceId": 86518,
     "sourceType": "competition"
    },
    {
     "datasetId": 5496847,
     "sourceId": 9107963,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5496920,
     "sourceId": 9108069,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8093914,
     "sourceId": 12801326,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8231.527793,
   "end_time": "2024-10-16T17:35:49.963587",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-16T15:18:38.435794",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
