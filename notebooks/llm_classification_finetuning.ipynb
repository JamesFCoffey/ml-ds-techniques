{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f93489c-6d0a-44b3-a8ee-871d7b865f1b",
   "metadata": {},
   "source": [
    "# LLM Classification Finetuning\n",
    "\n",
    "# Approach & rationale (why I pivoted to ‚Äúno-training‚Äù with big pretrained models)\n",
    "\n",
    "**TL;DR:** I started from the Keras starter (JAX backend, DeBERTa-v3-XS, L=512, \\~1.036 public LB). The plan was to ‚Äúdo everything right‚Äù: bigger model, longer context, 5-fold CV, augmentation, more epochs, tuned LR. In practice, with Kaggle GPUs, full fine-tuning at that scale was simply too slow. After squeezing most obvious bottlenecks, I pivoted to **inference-only strong pretrained LLM classifiers** (Llama3-8B, long context) with a lightweight similarity model. This runs in **\\~11 minutes on 2√óT4** and gets a much better score, without training.\n",
    "\n",
    "## What I tried first (and why it wasn‚Äôt fast enough)\n",
    "\n",
    "* Moved from DeBERTa-XS ‚Üí **DeBERTa-Small/Base** and **L > 512** to keep more of each prompt/response.\n",
    "* Kept the Keras design but optimized hard:\n",
    "\n",
    "  * **Pre-tokenized** per fold on CPU.\n",
    "  * **Single backbone call** per step (stack A/B along batch dim).\n",
    "  * **Static shapes** and **offline A/B swap** (avoid JAX recompiles).\n",
    "  * Mixed precision where possible; tried **JAX on P100** and **TF on 2√óT4**.\n",
    "\n",
    "* Reality check:\n",
    "\n",
    "  * **DeBERTa-Base OOM** on P100.\n",
    "  * DeBERTa-Small (L‚âà640), **5 folds √ó 6 epochs** ‚Üí \\~**48‚Äì49 hours** (both JAX/P100 and TF/2√óT4).\n",
    "    At that point the job is compute-bound; we‚Äôd need stronger hardware or fewer training knobs.\n",
    "\n",
    "## What I shipped instead (fast + strong)\n",
    "\n",
    "* **No training.** I run **pretrained preference-classification heads**:\n",
    "\n",
    "  * **Llama3-8B** with **long context (4096)**.\n",
    "  * Use **pipeline parallelism across 2 GPUs** for fast batched inference.\n",
    "* I add a **lightweight semantic-similarity model** (SentenceTransformer + FAISS) as a weak signal.\n",
    "* Net result: strong leaderboard score in **\\~11 minutes on 2√óT4**, zero fine-tune time.\n",
    "\n",
    "## Why this direction\n",
    "\n",
    "* With the available GPUs, **full fine-tuning at scale isn‚Äôt time-feasible** (even after heavy optimization).\n",
    "* **Pretrained LLM classifiers** already encode strong preference signals and **benefit from long context**.\n",
    "* **Inference-only** delivers a **much better score in minutes**, not days.\n",
    "\n",
    "## If I had more compute (future work)\n",
    "\n",
    "* Revisit **TF multi-GPU** fine-tuning of DeBERTa-Small/Base (or an encoder LLM) with L‚â•640, 5-fold CV, more epochs.\n",
    "* Try **LoRA/QLoRA** adapters on the LLM classifiers for a small, targeted fine-tune."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9310d37d-d912-41f7-a757-b92ffc3fc637",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7290623f-b098-4ba9-84aa-ebb88ce9b122",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T21:59:20.466950Z",
     "iopub.status.busy": "2025-08-18T21:59:20.466539Z",
     "iopub.status.idle": "2025-08-18T21:59:25.848990Z",
     "shell.execute_reply": "2025-08-18T21:59:25.847908Z",
     "shell.execute_reply.started": "2025-08-18T21:59:20.466923Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Remove packages that would force Torch back to 2.6.0 during dependency resolution\n",
    "%pip uninstall -y torchvision torchaudio || true\n",
    "\n",
    "# Point to your attached wheels dataset under /kaggle/input\n",
    "PACKAGES_DIR = \"/kaggle/input/offline-pytorch280-xformers032/wheels\"  # ‚Üê change to your dataset path\n",
    "\n",
    "# Install strictly from local wheels (no internet)\n",
    "%pip install --no-index --find-links=$PACKAGES_DIR \\\n",
    "    torch==2.8.0 xformers==0.0.32.post2 triton==3.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bff44b57-9989-4965-a082-cb7c6fa663c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T21:59:29.311782Z",
     "iopub.status.busy": "2025-08-18T21:59:29.311056Z",
     "iopub.status.idle": "2025-08-18T21:59:29.487535Z",
     "shell.execute_reply": "2025-08-18T21:59:29.486307Z",
     "shell.execute_reply.started": "2025-08-18T21:59:29.311747Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/lmsys-modules-0805 human_pref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aecc067-5a86-48dc-89fb-85ddb70fb59f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T21:59:30.662220Z",
     "iopub.status.busy": "2025-08-18T21:59:30.661866Z",
     "iopub.status.idle": "2025-08-18T21:59:30.675215Z",
     "shell.execute_reply": "2025-08-18T21:59:30.674309Z",
     "shell.execute_reply.started": "2025-08-18T21:59:30.662188Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.8.0+cu128\n",
      "Transformers: 4.52.4\n",
      "CUDA devices: 2\n",
      "GPU 0: Tesla T4\n",
      "GPU 1: Tesla T4\n",
      "\n",
      "Attached inputs under /kaggle/input:\n",
      " ‚Ä¢ /kaggle/input/deberta_v3\n",
      " ‚Ä¢ /kaggle/input/llm-classification-finetuning\n",
      " ‚Ä¢ /kaggle/input/lmsys-checkpoints-3-0805\n",
      " ‚Ä¢ /kaggle/input/lmsys-modules-0805\n",
      " ‚Ä¢ /kaggle/input/offline-pytorch280-xformers032\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import textwrap\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "print(\"CUDA devices:\", torch.cuda.device_count())\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}:\", torch.cuda.get_device_name(i))\n",
    "\n",
    "# Small speed hints\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# Offline inference only (no hub calls)\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "\n",
    "# Paths to your attached datasets (update if your names differ)\n",
    "MODEL_DIR = \"/kaggle/input/lmsys-checkpoints-3-0805\"\n",
    "MODULES_DIR = \"/kaggle/input/lmsys-modules-0805\"\n",
    "\n",
    "print(\"\\nAttached inputs under /kaggle/input:\")\n",
    "for p in sorted(glob.glob(\"/kaggle/input/*\")):\n",
    "    print(\" ‚Ä¢\", p)\n",
    "\n",
    "if not os.path.isdir(MODEL_DIR):\n",
    "    raise FileNotFoundError(\n",
    "        textwrap.dedent(f\"\"\"\n",
    "        MODEL_DIR not found: {MODEL_DIR}\n",
    "        ‚Üí Use the right sidebar ‚Üí Add data and attach the Llama‚Äë3 8B classifier checkpoint dataset.\n",
    "    \"\"\")\n",
    "    )\n",
    "\n",
    "# Make `human_pref` importable\n",
    "if os.path.isdir(MODULES_DIR) and MODULES_DIR not in sys.path:\n",
    "    sys.path.insert(0, MODULES_DIR)\n",
    "    print(\"Using helper modules from:\", MODULES_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d49034e-f22f-4379-bc0a-30a8638ddd05",
   "metadata": {},
   "source": [
    "# Prepare test files (original + swapped A/B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68c7f867-d525-495e-97cf-126be8a3d19f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T21:51:53.336108Z",
     "iopub.status.busy": "2025-08-18T21:51:53.335331Z",
     "iopub.status.idle": "2025-08-18T21:51:53.342189Z",
     "shell.execute_reply": "2025-08-18T21:51:53.341224Z",
     "shell.execute_reply.started": "2025-08-18T21:51:53.336076Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting prepare_test_file.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile prepare_test_file.py\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/kaggle/input/llm-classification-finetuning/test.csv\")\n",
    "# (Not scored here, but some helpers expect these columns.)\n",
    "for k in (\"winner_model_a\", \"winner_model_b\", \"winner_tie\"):\n",
    "    if k not in df.columns:\n",
    "        df[k] = 0\n",
    "\n",
    "df.to_parquet(\"test.parquet\", index=False)\n",
    "\n",
    "# Create swapped version (A<->B) for symmetry\n",
    "sw = df.copy()\n",
    "sw[\"response_a\"], sw[\"response_b\"] = sw[\"response_b\"], sw[\"response_a\"]\n",
    "sw.to_parquet(\"test_swap.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "440ae909-481b-415b-a4a0-548652a5fdb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T21:51:53.774359Z",
     "iopub.status.busy": "2025-08-18T21:51:53.774045Z",
     "iopub.status.idle": "2025-08-18T21:51:54.546519Z",
     "shell.execute_reply": "2025-08-18T21:51:54.545629Z",
     "shell.execute_reply.started": "2025-08-18T21:51:53.774334Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!python prepare_test_file.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eace29e-6c82-4048-a5d9-60d8de99e324",
   "metadata": {},
   "source": [
    "#  Llama‚Äë3 8B classifier inference (4k context, offline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c56058e-9413-4e13-bec1-9af07cb8c79c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T21:51:56.370626Z",
     "iopub.status.busy": "2025-08-18T21:51:56.370260Z",
     "iopub.status.idle": "2025-08-18T21:51:56.379993Z",
     "shell.execute_reply": "2025-08-18T21:51:56.379003Z",
     "shell.execute_reply.started": "2025-08-18T21:51:56.370595Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting predict_llama.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile predict_llama.py\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from xformers.ops.fmha.attn_bias import BlockDiagonalCausalMask\n",
    "\n",
    "from human_pref.models.modeling_llama import LlamaForSequenceClassification\n",
    "from human_pref.data.processors import ProcessorPAB\n",
    "from human_pref.data.dataset import LMSYSDataset\n",
    "from human_pref.data.collators import VarlenCollator, ShardedMaxTokensCollator\n",
    "from human_pref.utils import to_device\n",
    "\n",
    "MODEL_DIR = os.getenv(\"MODEL_DIR\", \"/kaggle/input/lmsys-checkpoints-3-0805\")\n",
    "MAX_LENGTH = int(os.getenv(\"MAX_LENGTH\", \"4096\"))  # longer context\n",
    "BATCH_SIZE = int(\n",
    "    os.getenv(\"BATCH_SIZE\", \"80\")\n",
    ")  # loader batch (split into micro-batches)\n",
    "MAX_TOKENS = int(os.getenv(\"MAX_TOKENS\", \"8192\"))  # sharded tokens budget\n",
    "NUM_WORKERS = int(os.getenv(\"NUM_WORKERS\", \"4\"))\n",
    "DTYPE = torch.float16\n",
    "\n",
    "assert torch.cuda.is_available(), \"GPU is required. Enable 2√óT4 in Kaggle settings.\"\n",
    "\n",
    "# ----------------------------------\n",
    "# Data pipeline (var‚Äëlen friendly)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "def make_loader(parquet_path: str):\n",
    "    \"\"\"Builds a var-length DataLoader for the LMSYS test parquet.\n",
    "\n",
    "    This constructs a tokenizer + `ProcessorPAB` that formats each example\n",
    "    (prompt, response_a, response_b) for Llama-style classification with long\n",
    "    context. It then creates an `LMSYSDataset` and a\n",
    "    `ShardedMaxTokensCollator`, which packs variable-length sequences into\n",
    "    micro-batches under a fixed token budget (`MAX_TOKENS`) for better GPU\n",
    "    utilization.\n",
    "\n",
    "    Args:\n",
    "      parquet_path: Absolute path to a `.parquet` file (e.g., \"test.parquet\"\n",
    "        or \"test_swap.parquet\") containing the competition columns:\n",
    "        [\"prompt\", \"response_a\", \"response_b\", ...].\n",
    "\n",
    "    Returns:\n",
    "      A `torch.utils.data.DataLoader` whose iterator yields **lists** of\n",
    "      micro-batches. Each micro-batch is a dict with keys required by the\n",
    "      var-length attention path:\n",
    "        - \"input_ids\": LongTensor of concatenated token ids\n",
    "        - \"cu_seqlens\": prefix sums of sequence lengths\n",
    "        - \"position_ids\": per-token rotary positions\n",
    "        - \"max_seq_len\": int (maximum sequence length in the micro-batch)\n",
    "        - \"seq_lens\": lengths of each sequence in the micro-batch\n",
    "    \"\"\"\n",
    "    # Load tokenizer from the local checkpoint directory.\n",
    "    tok = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "\n",
    "    # Suppress the benign warning when sequences exceed the model's original max.\n",
    "    tok.deprecation_warnings[\"sequence-length-is-longer-than-the-specified-maximum\"] = (\n",
    "        True\n",
    "    )\n",
    "\n",
    "    # Pack (prompt, A, B) into a single sequence suitable for sequence classification.\n",
    "    proc = ProcessorPAB(tokenizer=tok, max_length=MAX_LENGTH, support_system_role=True)\n",
    "\n",
    "    # Dataset reads rows from parquet and defers heavy work to the processor.\n",
    "    ds = LMSYSDataset(\n",
    "        csv_file=parquet_path,\n",
    "        query=None,\n",
    "        processor=proc,\n",
    "        include_swap=False,\n",
    "        is_parquet=True,\n",
    "    )\n",
    "\n",
    "    # Dynamic batching by tokens (not by examples) to keep memory usage stable.\n",
    "    coll = ShardedMaxTokensCollator(\n",
    "        max_tokens=MAX_TOKENS, base_collator=VarlenCollator()\n",
    "    )\n",
    "\n",
    "    # The DataLoader yields a list of micro-batches per outer batch, ready for pipeline run.\n",
    "    return DataLoader(\n",
    "        ds, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, collate_fn=coll\n",
    "    )\n",
    "\n",
    "\n",
    "# ----------------------------------\n",
    "# Model across 2 GPUs (pipeline split)\n",
    "# ----------------------------------\n",
    "\n",
    "n_gpus = torch.cuda.device_count()\n",
    "if n_gpus == 0:\n",
    "    raise SystemError(\"No GPU available. Please enable 2√óT4.\")\n",
    "\n",
    "# 32 transformer layers for Llama‚Äë3 8B\n",
    "NUM_LAYERS = 32\n",
    "if n_gpus >= 2:\n",
    "    device_map = {\n",
    "        \"model.embed_tokens\": \"cuda:0\",\n",
    "        \"model.norm\": \"cuda:1\",\n",
    "        \"score\": \"cuda:1\",\n",
    "    }\n",
    "    for i in range(NUM_LAYERS // 2):\n",
    "        device_map[f\"model.layers.{i}\"] = \"cuda:0\"\n",
    "    for i in range(NUM_LAYERS // 2, NUM_LAYERS):\n",
    "        device_map[f\"model.layers.{i}\"] = \"cuda:1\"\n",
    "else:\n",
    "    device_map = {\"\": \"cuda:0\"}\n",
    "\n",
    "model = LlamaForSequenceClassification.from_pretrained(\n",
    "    MODEL_DIR, torch_dtype=DTYPE, device_map=device_map\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# Build RoPE inv_freq per device (one per pipeline stage)\n",
    "cfg = model.config\n",
    "head_dim = cfg.hidden_size // cfg.num_attention_heads\n",
    "inv = 1.0 / (\n",
    "    cfg.rope_theta ** (torch.arange(0, head_dim, 2, dtype=torch.float32) / head_dim)\n",
    ")\n",
    "inv0 = inv.to(\"cuda:0\")\n",
    "inv1 = inv.to(\"cuda:1\" if n_gpus >= 2 else \"cuda:0\")\n",
    "\n",
    "# ----------------------------------\n",
    "# Pipeline run (micro‚Äëbatches)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "def run_one(parquet_path: str) -> torch.Tensor:\n",
    "    \"\"\"Runs pipelined two-GPU inference over one parquet file and returns probs.\n",
    "\n",
    "    This implements a simple two-stage pipeline across 2√óT4 when available:\n",
    "    stage-0 runs `forward_part1` (lower layers) on GPU0 while stage-1 runs\n",
    "    `forward_part2` (upper layers + classifier) on GPU1. The first micro-batch\n",
    "    \"primes\" the pipeline; the last micro-batch is \"flushed\" at the end.\n",
    "\n",
    "    Args:\n",
    "      parquet_path: Absolute path to the `.parquet` test file to score.\n",
    "\n",
    "    Returns:\n",
    "      A float32 `torch.Tensor` of shape `(N, 3)` on CPU containing softmax\n",
    "      probabilities for `[winner_model_a, winner_model_b, winner_tie]` in the\n",
    "      same order as rows in `parquet_path`.\n",
    "    \"\"\"\n",
    "    loader = make_loader(parquet_path)\n",
    "    outs = []\n",
    "    is_first = True  # True until I prime stage-1 with the first micro-batch\n",
    "    prev_hidden = None  # Activations handed from stage-0 -> stage-1\n",
    "    prev_info = None  # Attention/position metadata for the prior micro-batch\n",
    "\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(dtype=DTYPE):\n",
    "        for batch in loader:\n",
    "            # Each `batch` is a list of micro-batches produced by the sharded collator.\n",
    "            for micro in batch:\n",
    "                # Stage-0 always runs on cuda:0.\n",
    "                input_ids = to_device(micro[\"input_ids\"], \"cuda:0\")\n",
    "                info = dict(\n",
    "                    cu_seqlens=micro[\"cu_seqlens\"],\n",
    "                    position_ids=micro[\"position_ids\"],\n",
    "                    max_seq_len=micro[\"max_seq_len\"],\n",
    "                    # Block-diagonal mask for packed variable-length sequences.\n",
    "                    attn_bias=BlockDiagonalCausalMask.from_seqlens(micro[\"seq_lens\"]),\n",
    "                )\n",
    "                info = to_device(info, \"cuda:0\")\n",
    "\n",
    "                if is_first:\n",
    "                    # Prime the pipeline: produce hidden states on stage-0,\n",
    "                    # then move them (and metadata) to stage-1's device.\n",
    "                    prev_hidden = model.forward_part1(input_ids, info, inv0)\n",
    "                    prev_info, prev_hidden = to_device(\n",
    "                        [info, prev_hidden], \"cuda:1\" if n_gpus >= 2 else \"cuda:0\"\n",
    "                    )\n",
    "                    is_first = False\n",
    "                    continue\n",
    "\n",
    "                # While stage-1 finishes the previous micro-batch...\n",
    "                logits = model.forward_part2(prev_hidden, prev_info, inv1)\n",
    "                # ...stage-0 concurrently starts the current micro-batch.\n",
    "                hidden = model.forward_part1(input_ids, info, inv0)\n",
    "\n",
    "                # Slide the pipeline window forward and stash logits.\n",
    "                prev_info, prev_hidden = to_device(\n",
    "                    [info, hidden], \"cuda:1\" if n_gpus >= 2 else \"cuda:0\"\n",
    "                )\n",
    "                outs.append(logits.cpu())\n",
    "\n",
    "        # Flush the final micro-batch through stage-1 after the loop.\n",
    "        if prev_hidden is not None:\n",
    "            logits = model.forward_part2(prev_hidden, prev_info, inv1)\n",
    "            outs.append(logits.cpu())\n",
    "\n",
    "    if not outs:\n",
    "        # Empty input file or all examples filtered out.\n",
    "        return torch.empty((0, 3))\n",
    "\n",
    "    pred = torch.cat(outs, dim=0)\n",
    "    return pred.softmax(-1)  # (N, 3)\n",
    "\n",
    "\n",
    "# Original & swapped, then flip A/B back and average\n",
    "prob_a = run_one(\"test.parquet\")\n",
    "prob_b = run_one(\"test_swap.parquet\")\n",
    "prob = (prob_a + prob_b[:, [1, 0, 2]]) / 2.0\n",
    "\n",
    "np.save(\"prob_llama.npy\", prob.cpu().numpy())\n",
    "print(\"Saved prob_llama.npy with shape:\", prob.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72b95227-1d66-4ee5-85c1-e7a93bb8d945",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T21:51:59.055742Z",
     "iopub.status.busy": "2025-08-18T21:51:59.055395Z",
     "iopub.status.idle": "2025-08-18T21:54:09.291488Z",
     "shell.execute_reply": "2025-08-18T21:54:09.290617Z",
     "shell.execute_reply.started": "2025-08-18T21:51:59.055715Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-18 21:52:05.634052: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755553925.663983    1077 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755553925.675338    1077 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [01:48<00:00, 27.09s/it]\n",
      "/kaggle/working/predict_llama.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast(dtype=DTYPE):\n",
      "/kaggle/working/predict_llama.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast(dtype=DTYPE):\n",
      "Saved prob_llama.npy with shape: torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Run inference\n",
    "!python predict_llama.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9ef917-fed4-486d-8bb4-45cc740f3373",
   "metadata": {},
   "source": [
    "# Make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8ba600-9c24-45b9-b703-e873fe5444ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T21:54:09.293477Z",
     "iopub.status.busy": "2025-08-18T21:54:09.293107Z",
     "iopub.status.idle": "2025-08-18T21:54:09.773115Z",
     "shell.execute_reply": "2025-08-18T21:54:09.772233Z",
     "shell.execute_reply.started": "2025-08-18T21:54:09.293447Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136060</td>\n",
       "      <td>0.002493</td>\n",
       "      <td>0.983723</td>\n",
       "      <td>0.013783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211333</td>\n",
       "      <td>0.565638</td>\n",
       "      <td>0.136214</td>\n",
       "      <td>0.298148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1233961</td>\n",
       "      <td>0.107226</td>\n",
       "      <td>0.728414</td>\n",
       "      <td>0.164360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  winner_model_a  winner_model_b  winner_tie\n",
       "0   136060        0.002493        0.983723    0.013783\n",
       "1   211333        0.565638        0.136214    0.298148\n",
       "2  1233961        0.107226        0.728414    0.164360"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"test.parquet\")  # ids from non‚Äëswapped\n",
    "prob = np.load(\"prob_llama.npy\")\n",
    "\n",
    "sub = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": df[\"id\"],\n",
    "        \"winner_model_a\": prob[:, 0],\n",
    "        \"winner_model_b\": prob[:, 1],\n",
    "        \"winner_tie\": prob[:, 2],\n",
    "    }\n",
    ")\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99593a0d-0c41-40b5-943d-45683d72aa10",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "Single model only: Llama3‚Äë8B sequence classifier. No Gemma, no SentenceTransformer/FAISS.\n",
    "\n",
    "No external wheels: Everything runs with stock transformers and PyTorch shipped by Kaggle. We do not import xformers or human_pref.\n",
    "\n",
    "Longer context: MAX_LENGTH=4096 (‚â´ 512). If you OOM, first lower BATCH_SIZE; only then reduce MAX_LENGTH.\n",
    "\n",
    "2√óT4 automatically used: device_map=\"auto\" shards the model across both GPUs when available; otherwise it stays on a single GPU.\n",
    "\n",
    "Symmetry trick: Score original and A/B‚Äëswapped inputs, flip swapped logits back, and average to stabilize A/B preferences.\n",
    "\n",
    "Speed knobs: BATCH_SIZE (try 2‚Üí4 if memory allows), NUM_WORKERS (2‚Üí4). Keep dynamic padding on for throughput."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a8bdfc",
   "metadata": {
    "papermill": {
     "duration": 0.761993,
     "end_time": "2024-10-16T17:35:46.196467",
     "exception": false,
     "start_time": "2024-10-16T17:35:45.434474",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üìå | Reference\n",
    "\n",
    "* [[HCMUS][2025][24C15034] Ensemble Inference](https://www.kaggle.com/code/hoangvu132/hcmus-2025-24c15034-ensemble-inference)\n",
    "* [LMSYS: KerasNLP Starter](https://www.kaggle.com/code/addisonhoward/lmsys-kerasnlp-starter)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9809560,
     "isSourceIdPinned": false,
     "sourceId": 86518,
     "sourceType": "competition"
    },
    {
     "datasetId": 5496847,
     "sourceId": 9107963,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5496920,
     "sourceId": 9108069,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8093914,
     "sourceId": 12801326,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8231.527793,
   "end_time": "2024-10-16T17:35:49.963587",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-16T15:18:38.435794",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
